---
title: 'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks'
date: 2025-10-26
permalink: /posts/2025/10/blog-post-1/
---

## Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks

The paper introduces a general-purpose fine-tuning recipe for RAG models that aims to overcome these drawbacks by combining the strengths of **parametric memory** (the LLM's internal weights) and **non-parametric memory** (an external, searchable knowledge base).


## What is RAG and How Does it Work

RAG models: hybrid systems that combine a pre-trained **retriever** with a pre-trained **sequence-to-sequence (seq2seq) generator** and fine-tune them end-to-end.

The breakdown of the RAG architecture:

1.  **The Parametric Memory (Generator):** A pre-trained seq2seq model, specifically **BART-large** (a 400M parameter Transformer). This component is responsible for language generation and holds the model's implicit knowledge.
2.  **The Non-Parametric Memory (Retriever + Index):** Consists of a **pre-trained neural retriever** (DPR) and a **dense vector index of Wikipedia**. The Wikipedia index is split into 21 million disjoint 100-word chunks.
3.  **The Process:**
    * Given an input query ($x$), the **retriever** uses a query encoder to perform a **Maximum Inner Product Search (MIPS)** to find the top $K$ most relevant documents ($z$) from the Wikipedia index.
    * The **generator** then conditions on the original input and the retrieved documents to generate the output sequence ($y$).
    * The model marginalizes over the latent documents to get the final output probability, meaning the answer is generated by considering the possibilities from all the top documents.


## Two Formulations: RAG-Sequence and RAG-Token

* **RAG-Sequence:** Uses the **same retrieved document** to predict the entire output sequence. It treats the document as a single latent variable to be marginalized over.
* **RAG-Token:** More flexible, allowing it to draw a **different latent document for each target token**. Enables the generator to seamlessly combine information from multiple documents when creating an answer.
* Ex. When generating a Jeopardy question about Hemingway's books, it can switch documents to pull out the titles of two different novels.


## Breakthrough Results Across NLP Tasks

### Open-Domain Question Answering (QA)

RAG set the new state of the art on four open-domain QA tasks: **Natural Questions (NQ), WebQuestions (WQ), CuratedTrec (CT), and TriviaQA (TQA)**.

* **RAG-Sequence** achieved an Exact Match (EM) score of **44.5** on NQ, outperforming strong "Open-Book" methods like REALM (40.4) and DPR (41.5).
* The models generate answers, which is an advantage: they can produce a correct answer even if it's not present verbatim in any retrieved document, scoring **11.8% accuracy** in such cases on NQ where extractive models would score 0%.

### Language Generation

* **Specificity and Factuality:** Human evaluators overwhelmingly preferred RAG's generations over BART's, finding them to be more **factual and specific**. On factuality, RAG was better in $42.7\%$ of cases, versus BART's $7.1\%$.
* **Diversity:** RAG-Sequence and RAG-Token generations were found to be **significantly more diverse** than the BART baseline.


## Key Advantages of Retrieval-Augmented Generation

1.  **Fewer Trainable Parameters:** RAG-Sequence achieved strong QA performance with 626M total trainable parameters, significantly less than the T5-11B model (11 Billion trainable parameters) that was the best-performing closed-book model.
2.  **Easy Knowledge Updates (Hot-Swapping):** The non-parametric memory (the Wikipedia index) can be simply replaced—or "hot-swapped"—to update the model's world knowledge **without any retraining** This was demonstrated by switching between 2016 and 2018 Wikipedia indices to answer questions about world leaders who changed between those dates.
3.  **Improved Output Quality:** By grounding their responses in external text, RAG models generate language that is more **specific, diverse, and factual**, and they are less likely to "hallucinate".
